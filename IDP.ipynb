{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMY2raIfmaF5pVJsRadXJy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manthripranitha/IDP_1/blob/main/IDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHHa2BtYohXs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "td = pd.read_csv('/content/online payment fraud transaction detection.csv')\n",
        "print(td)"
      ],
      "metadata": {
        "id": "3WXr5S0pok_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(rows,columns)\n",
        "\n",
        "td.shape"
      ],
      "metadata": {
        "id": "VexBYMhMonmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data types\n",
        "\n",
        "td.dtypes"
      ],
      "metadata": {
        "id": "xqeHUO8PoqB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td.isnull()"
      ],
      "metadata": {
        "id": "J1n-K9CFosql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the missing values\n",
        "\n",
        "if td.isna().sum().sum() > 0:\n",
        "    print(f'There are {td.isna().sum().sum()} missing values in the dataset\\n')\n",
        "    td = td.dropna()\n",
        "    print('Shape after dropping missing values:', td.shape)\n",
        "else:\n",
        "    print('There are no missing values in the dataset.')\n"
      ],
      "metadata": {
        "id": "XOgeQ5ANov3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handling the outliers\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate Z-scores for all numeric columns\n",
        "z_scores = np.abs(stats.zscore(td.select_dtypes(include=np.number)))\n",
        "\n",
        "# Set a threshold to define outliers (e.g., z-score > 3)\n",
        "threshold = 3\n",
        "outliers = (z_scores > threshold).any(axis=1)\n",
        "\n",
        "# Create figure and axis for plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot: outliers vs non-outliers\n",
        "# The 'data' argument should be used to pass the DataFrame 'td'\n",
        "sns.scatterplot(data=td, x=td.index, y=td.select_dtypes(include=np.number).mean(axis=1), hue=outliers)\n",
        "# Customize the plot\n",
        "plt.title('Outliers Detected Using Z-Scores')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Mean Value of Numeric Features')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ov8mqnIPoyXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td.columns"
      ],
      "metadata": {
        "id": "7CKHuk50o1q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type feature\n",
        "td['type'].unique()"
      ],
      "metadata": {
        "id": "xezX4K9Do4C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_counts = td.groupby('type').size()\n",
        "print(type_counts)\n"
      ],
      "metadata": {
        "id": "OfGSiJulo6zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#barplot to display the type vs counts\n",
        "\n",
        "# Count the occurrences of each type\n",
        "type_counts = td['type'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.bar(type_counts.index, type_counts.values, color='skyblue')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Type vs Counts')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Counts')\n",
        "plt.xticks(rotation=45)  # Rotate x labels for better readability\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AjcB_TiHo9O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install colorama\n"
      ],
      "metadata": {
        "id": "W8bYtT-1pAPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Fore, Style\n",
        "\n",
        "# Display the type that is used more and which is used less\n",
        "most_common_type = type_counts.idxmax()\n",
        "most_common_count = type_counts.max()\n",
        "least_common_type = type_counts.idxmin()\n",
        "least_common_count = type_counts.min()\n",
        "\n",
        "# Print with colors\n",
        "print(Fore.GREEN + f'Most Common Type: {most_common_type} (Count: {most_common_count})' + Style.RESET_ALL)\n",
        "print(Fore.RED + f'Least Common Type: {least_common_type} (Count: {least_common_count})' + Style.RESET_ALL)"
      ],
      "metadata": {
        "id": "su3aDUSbpClM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Sample DataFrame for demonstration\n",
        "data = pd.DataFrame({\n",
        "    'type': ['CASH_OUT', 'PAYMENT', 'CASH_IN', 'TRANSFER', 'DEBIT', 'CASH_OUT', 'PAYMENT']\n",
        "})\n",
        "\n",
        "# Convert types to numeric values using factorize\n",
        "data['type_numeric'], unique_types = pd.factorize(data['type'])\n",
        "\n",
        "# Get counts of each type after conversion\n",
        "type_counts = data['type'].value_counts()\n",
        "\n",
        "# Display the DataFrame with numeric values and the counts\n",
        "print(data)\n",
        "print(\"\\nCounts of each type:\")\n",
        "print(type_counts)\n"
      ],
      "metadata": {
        "id": "2SSvuAi_pE5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels for isfraud\n",
        "td['isFraud'].unique()"
      ],
      "metadata": {
        "id": "BKPVY5m2pH1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the occurrences of each target value\n",
        "target_counts = td['isFraud'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(target_counts.index, target_counts.values, color=['lightcoral', 'lightgreen'])\n",
        "\n",
        "# Add titles and labels\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Target Counts \\n (isn\\'t Fraud = 0 || is Fraud = 1)')\n",
        "\n",
        "# Add value annotations on top of the bars\n",
        "for i, count in enumerate(target_counts.values):\n",
        "    plt.text(i, count + 5, str(count), ha='center', fontsize=12)\n",
        "\n",
        "# Set x-ticks to ensure they are labeled correctly\n",
        "plt.xticks(target_counts.index, ['Not Fraud (0)', 'Fraud (1)'])\n",
        "\n",
        "# Enable the grid on the y-axis for better readability\n",
        "plt.grid(axis='y', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ReZsbT60pKrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing unnecessary columns\n",
        "\n",
        "td.drop(['customer_starting_transaction', 'Recipient_transaction'], axis=1, inplace=True)\n",
        "td.shape"
      ],
      "metadata": {
        "id": "KizyS9aPpNaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "\n",
        "# Assuming 'type' is the problematic column in DataFrame 'td'\n",
        "# Convert 'type' column to numeric using pd.factorize\n",
        "\n",
        "# Create a mapping of type values to numeric values\n",
        "type_mapping = {type_val: i for i, type_val in enumerate(td['type'].unique())}\n",
        "\n",
        "# Apply the mapping to the 'type' column\n",
        "td['type_numeric'] = td['type'].map(type_mapping)\n",
        "\n",
        "# Now you can calculate the correlation, but only on numeric columns\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_td = td.select_dtypes(include=['number'])\n",
        "\n",
        "correlation_matrix = numeric_td.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "_ksXDlKJpPv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix, but only on numeric columns\n",
        "correlation_matrix = td.select_dtypes(include=np.number).corr()\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Create a heatmap using imshow\n",
        "cax = plt.imshow(correlation_matrix, interpolation='nearest', cmap='Spectral')\n",
        "\n",
        "# Add a colorbar\n",
        "plt.colorbar(cax)\n",
        "\n",
        "# Set the ticks and labels\n",
        "plt.xticks(np.arange(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)\n",
        "plt.yticks(np.arange(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
        "\n",
        "# Add annotations to the heatmap\n",
        "for (i, j), val in np.ndenumerate(correlation_matrix.values):\n",
        "    plt.text(j, i, f\"{val:.1f}\", ha='center', va='center', color='black')\n",
        "\n",
        "# Set title\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jWuPGzcZpShi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "correlation = td['oldbalance'].corr(td['newbalance'])\n",
        "\n",
        "# Display the correlation\n",
        "print(f\"Correlation between 'oldbalance' and 'newbalance': {correlation}\")\n"
      ],
      "metadata": {
        "id": "JRS02NlhpVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#line plot\n",
        "\n",
        "\n",
        "# Sort data for line plot\n",
        "data_sorted = td.sort_values('oldbalance')\n",
        "\n",
        "# Create a line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data_sorted['oldbalance'], data_sorted['newbalance'], marker='o', linestyle='-', color='b', label='Data')\n",
        "plt.xlabel('oldbalance')\n",
        "plt.ylabel('newbalance')\n",
        "plt.title('Line Plot of oldbalance vs. newbalance')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SeJL20MqpX-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping unnecessary features\n",
        "del td['isFlaggedFraud']\n",
        "del td['step']\n",
        "td.columns\n"
      ],
      "metadata": {
        "id": "vC4Juj3_pa1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del td['type_numeric']"
      ],
      "metadata": {
        "id": "vYcR_OJDpdHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td.shape\n"
      ],
      "metadata": {
        "id": "UOeMz7OHpfar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(td)"
      ],
      "metadata": {
        "id": "2B1l6jpopiNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td.describe().T"
      ],
      "metadata": {
        "id": "-rsk3e_uplAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data and target\n",
        "\n",
        "# X Data\n",
        "X = td.drop(['isFraud'], axis=1)\n",
        "print('X shape is : ' , X.shape)\n",
        "print()\n",
        "\n",
        "# y Data\n",
        "y = td['isFraud']\n",
        "print('y shape is : ' , y.shape)"
      ],
      "metadata": {
        "id": "_nRDmoYspnS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset train_test_split function from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are defined\n",
        "# Split the data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Splitted Data\n",
        "shapes = (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "print('Shapes of the splitted data (X_train, X_test, y_train, y_test):', shapes)\n"
      ],
      "metadata": {
        "id": "82kV0ekopp9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Assuming X and y are defined\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "# Assuming 'type' is the column containing 'PAYMENT' and other categorical values\n",
        "categorical_features = ['type']  # Replace with your actual categorical columns\n",
        "numerical_features = X_train.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Create a ColumnTransformer to apply OneHotEncoding to categorical features\n",
        "# and MinMaxScaler to numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
        "    ])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "X_test_scaled = preprocessor.transform(X_test)\n",
        "\n",
        "# Display the shape of scaled data\n",
        "print('Shapes of the scaled data (X_train_scaled, X_test_scaled):', (X_train_scaled.shape, X_test_scaled.shape))"
      ],
      "metadata": {
        "id": "L_so2HBApsb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                    #DECISION TREE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "Model_DT = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model on the scaled training data\n",
        "Model_DT.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test data\n",
        "y_pred_DT = Model_DT.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate using accuracy_score\n",
        "Train_Accuracy = accuracy_score(y_train, Model_DT.predict(X_train_scaled))\n",
        "Test_Accuracy = accuracy_score(y_test, y_pred_DT)\n",
        "\n",
        "# Print results\n",
        "print(f\"Training Accuracy: {Train_Accuracy * 100:.2f} %\")\n",
        "print(f\"Testing Accuracy: {Test_Accuracy * 100:.2f} %\")\n"
      ],
      "metadata": {
        "id": "mFCmhHLHpu_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate the confusion matrix\n",
        "CM = confusion_matrix(y_true=y_test, y_pred=y_pred_DT)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(CM, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jcK4eATJpx8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_DT))"
      ],
      "metadata": {
        "id": "5uOv3pU6p0ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # Importing necessary functions\n",
        "Accuracy_DT = accuracy_score(y_test, y_pred_DT)\n",
        "print(f'➤➤➤ Accuracy Score : {Accuracy_DT * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Precision = TP / (TP + FP)\n",
        "Precision_DT = precision_score(y_test, y_pred_DT)\n",
        "print(f'➤➤➤ Precision Score : {Precision_DT * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Recall = TP / (TP + FN)\n",
        "Recall_DT = recall_score(y_test, y_pred_DT)\n",
        "print(f'➤➤➤ Recall Score : {Recall_DT * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# F1 Score = 2 × ((Precision * Recall) / (Precision + Recall))\n",
        "F1_Score_DT = f1_score(y_test, y_pred_DT)\n",
        "print(f'➤➤➤ F1 Score : {F1_Score_DT * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "ROC_AUC_DT = roc_auc_score(y_test, y_pred_DT)\n",
        "print(f'➤➤➤ AUC_ROC : {ROC_AUC_DT * 100 : .2f} %\\n')"
      ],
      "metadata": {
        "id": "87BLvGzep2zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# New variables for scores (you can assign new values as needed)\n",
        "model_scores = [0.85, 0.75, 0.65, 0.70, 0.80]  # Example values for the metrics\n",
        "score_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']  # Adjusted variable names\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(score_names, model_scores, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)  # Set y-axis limits\n",
        "plt.axhline(y=0.5, color='gray', linestyle='--')  # Add a horizontal line for reference\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wNGhZbiqp5sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                         #Naive Bayes\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Split into train, validation, and holdout\n",
        "X_train_val, X_holdout, y_train_val, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "# ... (Your feature engineering code here) ...\n",
        "\n",
        "# Assuming 'X' is your original feature data\n",
        "# Create a scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select only numerical features for scaling\n",
        "numerical_features = X_train.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Fit the scaler on the numerical features of the training data and transform\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[numerical_features]),\n",
        "                               columns=numerical_features,\n",
        "                               index=X_train.index)\n",
        "\n",
        "# Transform the validation and holdout data using the trained scaler\n",
        "X_val_scaled = pd.DataFrame(scaler.transform(X_val[numerical_features]),\n",
        "                             columns=numerical_features,\n",
        "                             index=X_val.index)\n",
        "\n",
        "X_holdout_scaled = pd.DataFrame(scaler.transform(X_holdout[numerical_features]),\n",
        "                                 columns=numerical_features,\n",
        "                                 index=X_holdout.index)  # Scale the holdout data before prediction\n",
        "\n",
        "\n",
        "# 2. Train and tune on train/validation\n",
        "model = GaussianNB()\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train_scaled, y_train) # This line is added to fit the model\n",
        "# ... (Your hyperparameter tuning code here - use X_train_scaled, y_train, X_val_scaled, y_val) ...\n",
        "\n",
        "# 3. Evaluate on the holdout set\n",
        "y_pred_holdout = model.predict(X_holdout_scaled)\n",
        "holdout_accuracy = accuracy_score(y_holdout, y_pred_holdout)\n",
        "print(f\"Holdout Accuracy: {holdout_accuracy}\")"
      ],
      "metadata": {
        "id": "Pzs6GflIp8eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_holdout, y_pred_holdout)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn's heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for Holdout Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-z3rkDbwp-2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test[numerical_features]),\n",
        "                              columns=numerical_features,\n",
        "                              index=X_test.index)\n",
        "y_pred_NB = model.predict(X_test_scaled) # Generate the predictions using the NB model\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred_NB))"
      ],
      "metadata": {
        "id": "hg01lS9oqBc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # Importing necessary functions\n",
        "Accuracy_NB = accuracy_score(y_test, y_pred_NB)\n",
        "print(f'➤➤➤ Accuracy Score : {Accuracy_NB * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Precision = TP / (TP + FP)\n",
        "Precision_NB = precision_score(y_test, y_pred_NB)\n",
        "print(f'➤➤➤ Precision Score : {Precision_NB * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Recall = TP / (TP + FN)\n",
        "Recall_NB = recall_score(y_test, y_pred_NB)\n",
        "print(f'➤➤➤ Recall Score : {Recall_NB * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# F1 Score = 2 × ((Precision * Recall) / (Precision + Recall))\n",
        "F1_Score_NB = f1_score(y_test, y_pred_NB)\n",
        "print(f'➤➤➤ F1 Score : {F1_Score_NB * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "ROC_AUC_NB = roc_auc_score(y_test, y_pred_NB)\n",
        "print(f'➤➤➤ AUC_ROC : {ROC_AUC_NB * 100 : .2f} %\\n')"
      ],
      "metadata": {
        "id": "3w4w45mgqD-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scores = [Accuracy_NB, Precision_NB, Recall_NB, F1_Score_NB, ROC_AUC_NB]\n",
        "Score_Names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.pie(Scores, labels=Score_Names, autopct='%1.2f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_cc_sphQqGkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "                                    #RANDOM FOREST\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "Model_RF = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the scaled training data\n",
        "Model_RF.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test data\n",
        "y_pred_RF = Model_RF.predict(X_test_scaled)\n",
        "\n",
        "# Quick evaluation\n",
        "Train_Accuracy_RF = Model_RF.score(X_train_scaled, y_train)\n",
        "Test_Accuracy_RF = Model_RF.score(X_test_scaled, y_test)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f'Training accuracy: {Train_Accuracy_RF * 100:.2f} %')\n",
        "print(f'Testing accuracy: {Test_Accuracy_RF * 100:.2f} %')\n"
      ],
      "metadata": {
        "id": "GJQcl4v4qJMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate the confusion matrix\n",
        "CM = confusion_matrix(y_true=y_test, y_pred=y_pred_RF)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(CM, annot=True, fmt='d', cmap='plasma', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3qXU7e_qLtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_RF))"
      ],
      "metadata": {
        "id": "Pjo3i6ixqOdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # Importing necessary functions\n",
        "Accuracy_RF = accuracy_score(y_test, y_pred_RF)\n",
        "print(f'➤➤➤ Accuracy Score : {Accuracy_RF * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Precision = TP / (TP + FP)\n",
        "Precision_RF = precision_score(y_test, y_pred_RF)\n",
        "print(f'➤➤➤ Precision Score : {Precision_RF * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Recall = TP / (TP + FN)\n",
        "Recall_RF = recall_score(y_test, y_pred_RF)\n",
        "print(f'➤➤➤ Recall Score : {Recall_RF * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# F1 Score = 2 × ((Precision * Recall) / (Precision + Recall))\n",
        "F1_Score_RF = f1_score(y_test, y_pred_RF)\n",
        "print(f'➤➤➤ F1 Score : {F1_Score_RF * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "ROC_AUC_RF = roc_auc_score(y_test, y_pred_RF)\n",
        "print(f'➤➤➤ AUC_ROC : {ROC_AUC_RF * 100 : .2f} %\\n')"
      ],
      "metadata": {
        "id": "oA7JO1WUqRAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scores = [Accuracy_RF, Precision_RF, Recall_RF, F1_Score_RF, ROC_AUC_RF]\n",
        "Score_Names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.pie(Scores, labels=Score_Names, autopct='%1.2f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mk-XoAQuqTwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "Model_LR = LogisticRegression()\n",
        "Model_LR.fit(X_train_scaled, y_train)\n",
        "y_pred_LR = Model_LR.predict(X_test_scaled)\n",
        "\n",
        "# Quick evaluation\n",
        "Train_Accuracy = Model_LR.score(X_train_scaled, y_train)\n",
        "Test_Accuracy = Model_LR.score(X_test_scaled, y_test)\n",
        "print(f'Training accuracy: {Train_Accuracy*100:.2f} %')\n",
        "print(f'Testing accuracy: {Test_Accuracy*100:.2f} %')"
      ],
      "metadata": {
        "id": "vQmr25nIqWYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Assuming 'y_test' contains the true labels and has information about fraud status\n",
        "# Get unique labels from y_test\n",
        "display_labels =  list(set(y_test)) # or np.unique(y_test)\n",
        "\n",
        "CM = confusion_matrix(y_true=y_test, y_pred=y_pred_LR)\n",
        "ConfusionMatrixDisplay(CM, display_labels=display_labels).plot()\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XvAmggqjqYtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_LR = accuracy_score(y_test, y_pred_LR)\n",
        "print(f'➤➤➤ Accuracy Score : {Accuracy_LR * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Precision = TP / (TP + FP)\n",
        "Precision_LR = precision_score(y_test, y_pred_LR)\n",
        "print(f'➤➤➤ Precision Score : {Precision_LR * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# Recall = TP / (TP + FN)\n",
        "Recall_LR = recall_score(y_test, y_pred_LR)\n",
        "print(f'➤➤➤ Recall Score : {Recall_LR * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "# F1 Score = 2 × ((Precision * Recall) / (Precision + Recall))\n",
        "F1_Score_LR = f1_score(y_test, y_pred_LR)\n",
        "print(f'➤➤➤ F1 Score : {F1_Score_LR * 100 : .2f} %\\n')\n",
        "\n",
        "\n",
        "ROC_AUC_LR = roc_auc_score(y_test, y_pred_LR)\n",
        "print(f'➤➤➤ AUC_ROC : {ROC_AUC_LR * 100 : .2f} %\\n')"
      ],
      "metadata": {
        "id": "JzlkKZTCqbHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scores = [Accuracy_LR, Precision_LR, Recall_LR, F1_Score_LR, ROC_AUC_LR]\n",
        "Score_Names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.pie(Scores, labels=Score_Names, autopct='%1.2f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EzAeCDOzqdm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = pd.DataFrame({'Model': ['Decision Tree','Naive Bayes','Logistic Regression','Random forest'],\n",
        "                           'Accuracy': [(Accuracy_DT*100),(Accuracy_NB*100),(Accuracy_LR*100), (Accuracy_RF*100)]})\n",
        "\n",
        "evaluation"
      ],
      "metadata": {
        "id": "UN0XRoYrqgMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}